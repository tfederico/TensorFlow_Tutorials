{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Neural Networks in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test if our installation of TensorFlow functions as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "deep_learning = tf.constant('Deep Learning')\n",
    "\n",
    "session = tf.Session()\n",
    "session.run(deep_learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant(2)\n",
    "b = tf.constant(3)\n",
    "\n",
    "multiply = tf.multiply(a, b) # in the book you will find tf.mult(a, b),\n",
    "                             # but it refers to an older version of TensorFlow\n",
    "\n",
    "session.run(multiply)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and Manipulating TensorFlow Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use variables to represent the parameters of the model. TF variables are in-memory buffers that contain tensors; but unlike normal tensors that are only instantiated when a graph is run and that are immediately wiped clean afterwards, variables survive across multiple executions of a graph. As a result, TF variables have the following 3 properties:\n",
    " - Variables must be explicitly inizialised before a graph is used for the first time\n",
    " - We can use gradient methods to modify variables after each iteration as we search for a model's optiaml parameters setting\n",
    " - We can save the values stored in variables to disk and restore them for later use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start off by initialising a variable that describes the weights connecting neurons between two layers of a feed-forward neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = tf.Variable(tf.random_normal([300, 200], stddev=0.5), name=\"weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we pass two arguments to `tf.Variable`. The first, `tf.random_normal` is an operation that produces a tensor initialised using a normal distribution with standard deviation 0.5.We have specified that this tensor is of size 300 x 200, implying that the weights connect a layer with 300 neurons to a layer with 200 neurons. \n",
    "We have also passed a name to our `tf.Variable`. The name is a uniques identifier that allows us to refer to the appropriate node in the computation graph. In this case 'weights' is meant to be _trainable_ or, in other words, we will automatically compute and apply gradients to `weights`. If `weights` is not meant to be trainable, we may pass an optional flag when we call `tf.Variable`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = tf.Variable(tf.random_normal([300, 200], stddev=0.5), name='weights', trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several other methods to initialise a TensorFlow variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common tensors from the TensorFlow API docs\n",
    "\n",
    "shape = [10, 10]\n",
    "\n",
    "tf.zeros(shape, dtype=tf.float32, name=None)\n",
    "tf.ones(shape, dtype=tf.float32, name=None)\n",
    "tf.random_normal(shape, mean=0.0, stddev=1.0,               # Returns a tensor of the specified shape\n",
    "                 dtype=tf.float32, seed=None, name=None)    # filled with random normal values.\n",
    "                                                            # Alternatively, tf.random.normal(...)\n",
    "tf.truncated_normal(shape, mean=0.0, stddev=1.0,\n",
    "                    dtype=tf.float32, seed=None, name=None) # filled with random truncated (values \n",
    "                                                            # whose magnitude is more than 2 standard\n",
    "                                                            # deviations from the mean are dropped and re-picked).\n",
    "                                                            # Alternatively, tf.truncated_normal(...)\n",
    "tf.random_uniform(shape, minval=0, maxval=None, \n",
    "                    dtype=tf.float32, seed=None, name=None) # filled with random uniformed values.\n",
    "                                                            # Alternatively, tf.random.uniform(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we call `tf.Variable`, three operations are added to the computation graph:\n",
    "- The operation producing the tensor we use to initialise our variable\n",
    "- The `tf.assign` operation, which is responsible for filling the variable with the initalising tensor prior the use of the variable\n",
    "- The variable operation, which holds the current value of the variable\n",
    "\n",
    "Before we use any TensorFlow variable, the `tf.assign` operation must be run so that the variable is appropriately initialised with the desired value. We can do this by running `tf.initialize_all_variables()`, which will trigger all of the `tf.assign` operations in our graph. We can also selectively initialise only certain variables in our computational graph using the `tf.initialize_variables(var1, var2, ...)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a high level, TF operations represent abstract transformations that are applied to tensors in the computational graph. Operations may have attributes that may be supplied a priori or are inferred at runtime. Just as variables are named, operations may also be supplied with an optional name attribute for easy reference into the computational graph.\n",
    "\n",
    "An operation consists of one or more **kernels**, which represent device-specific implementations. For example, an operation may have separate CPU and GPU kernels because it can be more efficiently expressed on a GPU.\n",
    "\n",
    "| **Category** | **Examples** |\n",
    "| -------------------------------------- | ------------------------------------------------------------------------ |\n",
    "| Element-wise mathematical operations | `Add, Subtract, Multiply, Divide, Exp, Log, Greater, Less, Equal, ...` |\n",
    "| Array operations | `Concat, Slice, Split, Constant, Rank, Shape, Shuffle, ...` |\n",
    "| Matrix operations | `MatMul, MatrixInverse, MatrixDeterminant, ...` |\n",
    "| Stateful operations | `Variable, Assign, AssignAdd, ...` |\n",
    "| Neural networks building blocks | `SoftMax, Sigmoid, ReLU, Convolution2D, MaxPool, ...` |\n",
    "| Checkpoint operations | `Save, Restore` |\n",
    "| Queue and synchronization operations | `Enqueue, Dequeue, MutexAcquire, MutexRelease, ...` |\n",
    "| Control flow operations | `Merge, Switch, Enter, Leave, NextIteration` |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
