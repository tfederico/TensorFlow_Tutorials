{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "data_path = \"data/\"\n",
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(data_path+pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the logistic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-8b25b93d2ce8>:26: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32, \\\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # For the L2 loss\n",
    "  beta_regul = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(tf.truncated_normal([image_size * image_size, \\\n",
    "                                             num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "          labels=tf_train_labels, logits=logits) + \\\n",
    "            beta_regul * tf.nn.l2_loss(weights))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights) + \\\n",
    "                                   biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/federico/anaconda3/envs/dlcv/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:189: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Minibatch loss at step 0: 24.665695\n",
      "Minibatch accuracy: 1.6%\n",
      "Validation accuracy: 6.9%\n",
      "Minibatch loss at step 500: 2.737254\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 76.1%\n",
      "Minibatch loss at step 1000: 1.975232\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 78.2%\n",
      "Minibatch loss at step 1500: 1.297055\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 79.8%\n",
      "Minibatch loss at step 2000: 1.140291\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 80.7%\n",
      "Minibatch loss at step 2500: 0.829868\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 81.3%\n",
      "Minibatch loss at step 3000: 0.673313\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 82.1%\n",
      "Test accuracy: 88.6%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, \n",
    "                 tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction], \\\n",
    "                                    feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), \\\n",
    "                                                     valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The L2 regularization introduces a new meta parameter that should be tuned. Since I do not have any idea of what should be the right value for this meta parameter, I will plot the accuracy by the meta parameter value (in a logarithmic scale)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 3001\n",
    "regul_val = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "accuracy_val = []\n",
    "\n",
    "for regul in regul_val:\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.initialize_all_variables().run()\n",
    "        for step in range(num_steps):\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            \n",
    "            feed_dict = {tf_train_dataset: batch_data, \\\n",
    "                         tf_train_labels: batch_labels, beta_regul: regul}\n",
    "            _, l, predictions = session.run([optimizer, \\\n",
    "                                             loss, train_prediction], \\\n",
    "                                                feed_dict=feed_dict)\n",
    "        accuracy_val.append(accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEMCAYAAAAoB2Y1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xd8FVX6+PHPk04KgVACCSX03juIomJXmg1WFHURse666+66rn3Vdf3qfnf9qSCLrqKi0hGwYAuLSpHeUQIhJHRCCAkJaef3x0z8XkNCyr3J3PK8X6+8kjsz58wz9577ZO45c8+IMQallFKBIcjpAJRSStUdTfpKKRVANOkrpVQA0aSvlFIBRJO+UkoFEE36SikVQDTpK68nIhEiYkSkhdOxVJeIrBaRiW6UTxGRIR6OKVxEckQkwZP1utT/vyIy1f77ShHZ44E6axyziDwtIq9WYbvXReT2GgXoQzTpe4DdGEt/SkQkz+XxLW7U61bCUL7PGNPOGLPKnTrKtiNjzFljTLQx5qD7EZ6zr0TgBuAtT9Zb1ZjL+ydjjHnSGHN/FXbzIvCUiAS7E6u306TvAXZjjDbGRANpwHUuy953Or7aIiIhTsfgLm89Bm+NqwruBBYZYwqcDqS6jDGpwAHgKodDqVWa9OuAiASLyOMisldEjovI+yLSwF4XJSIfikimiGSJyBoRaSgiLwMDgJn2J4aXy6k3RETmi8gRu+w3ItLJZX2UiLwiIgdE5JSIrChNJiIywj4DPCUiaSLyK3v5L84KRWSqiHxp/13azXKPiKQA2+zl00QkXUSyRWStiAwuE+OT9rFni8gPItJMRN4UkefKHM8Xpd0CFRgjIqkickxEnhNLpF1vB5d6WojImdLnuMw+porI1yLymoicBB6xl98tIrvt12GZfcZaWuYaEfnJfo7/6focicgLIjLTZdvOIlJUXvD2umR7H8dE5B0RiXFZf1hEHhaR7UC2y7IL7Dbk+oky134tmolIExH51K4zU0QWi0hzu/w57UjKdJeJSJyIzLbL7xORP4qIuDxfX9ntKEus7qaR53mNrgJWVLRSRHqIyEq7ri0icpXLuqb2cWTbz/EL5bS90phHi8guETltt+8HRaQRsBBo6/I8NSrnNSq37duSgWvOc3y+zxijPx78AVKBkWWWPQKsBBKACOBt4D/2ut8A84B6QAjWGzTKXrcamHiefYUAk4Bou95pwGqX9W8Cy4FmQDAw3P7dHsgBrrfraAL0Km+fwFTgS/vvCMAAy4AGQD17+W1AQyAU+AvW2VKove5xYKO9zyCgj132QmAfIPZ2CcAZIK6c4yzd7+d22TbA3tI4sboSnnbZ/k/A3Aqes6lAEXCX/VzUA8YDO4GO9jE8C3xjb9/Mfq6utdf9ESh02fcLwEyX+jsDRS6PV7ts2xm4BAiz610NvOCy7WHgB/u5qOey7IJyjuMfwJf2McQDo+1jiQUWAx+WF0OZ57OF/XgOMNduR+3t1+UWl+er0H6Ng4GHgNTztMnTQA+Xx1cCe1z2ux/4vf1cXmE/t23s9YuAWfZx9AQOcW7bK435BDDQ/rsR0Kfs/lxi+Pk14jxt317/K+B7p/NIbf44HoC//VB+0t8HDHN53AYrwQlwL9aZUfdy6jpv0i9n+2ZAif0GCbXfrJ3K2e5p4IMK6qhK0h96nhjEPrZO9uP9wBUVbLcXGG4/fhhYUEGdpfsd4bLsd8Ay+++LXN/owFZgVAV1TQV+LLPsm9IkZz8ufe7igSnY/wDsdUHAUWqQ9MuJZTywyuXxYeBXZbY5J+ljJeA9lPMP0l4/GDh0ntf05wQKhAPFQFuX9b8BPnN5vra5rIuzyzYoZ7/B9rokl2WuSf8yuz2Iy/qFWCdFEXbbbe2y7qVy2l5p0j8C3AHElImhsqRfYdu3118H7Kjqe84Xf7R7p5bZH5NbAp/YH2mzsM58g7DOUN7ESvrz7C6S56WKA0l218lLpV0nwC6sZNoIaI51JrO3nKItgRQ3DutAmTj+bHeNnAJOYr1BG9vHnljevoz1DpsFlHYlTQTercZ+92OdEQP8FwgWkSEi0hvr2D+tavxAa2C6y+tzDOvTQAt7Hz9vb4wpATIqibNcIpIgInNFJMN+vWYCjSuJrWwdA4GXgTHGmEx7WYyIvGV3VWRjfborW29FmmG1xTSXZfuxXrdSh13+PmP/ji5bkTGmGOtMP6bsOlsCkGa/9mX31Qyr7aa7rDvfczEG62w9ze6u63+ebV1V1vZjgKwq1uWTNOnXMruBZwCXGGMauPxEGGOOG+uqhCeMMZ2xujxuxDoDBOvM5nzuAC4HLsb6WN/ZXi5YH42LgLbllDsAtKugzlwg0uVxs/IOq/QPEbkMeAAYi9X1EgfkYZ3NlR57RfuaBdwgIv2w3ozLKtiuVEuXv1sBB+GcfyC3YnVtFJ6nnrLP6wHg9jKvTz1jzHqs5/HnS0VFJIhfJsSqPF+l/sfevrsxpj4wGeu1Ol9sP7P76RcAk40x21xWPWLHOMCu9/Iy9Z6vHR3GOsNu5bKsFTX8xwZsweomK8/BMvtx3ddhrDhdn9uWVMAYs8oYcy3Wp7HlwAelqyqJ73xtH6ALsLmSOnyaJv26MR14QURaws8DVtfZf48Uka52MsnGStTFdrkjlJ+0S8UA+Vj9m1FYfdEA2ElvFvAvEYm3BwIvsD9FzAKuFZGx9vImItLTLroJKxFHiEhn4PZKji0GqyvkGFZf9TNYZ/qlZgLPi0hbsfQRe4DVGLMX2AH8B/jIVH7Fx59EJFZEkoD7gY9c1s0CbgIm2H9Xx3TgMbEHwcUaSL/eXvcxMEhErhZrEPx3WOMXpTYBF4tIoog0xBpPqEgMVn9ytoi0suuqEhEJw+oKecMYs7ices8AWSLSGHiszPoK25Ex5qxd7/NiDfy3w+reea+qsZXxCVZ3W3lWAkEi8lv7U+plWP+g5hpj8oElwNN22+uO1b9+DjvO8SJSH6vtneaX75mmInLOJxHb+do+duzn+5To8zTp140XsQbdvhaR08D3QF97XSLWwNtprKthPsEaWAP4X+A2ETkpIi+WU++bWMn2MFY/9rdl1j+I9VF2I9Y/hr9inYGnYA38PYrVHbMO6OYSa4hd7wwqf/MvwepeScHqSjpuly31AtYZ/NdY/9SmY/Ujl3oH6EHlXTvY9Wy2453rGpt9TLuB08aYtVWo62fGmA+AV4EFdvfIJqz+Z4wxh7D+kbxiH1sLrOf6rEtMS7H+ea3GGoysyBPABcAprEQ7vxphtgUGYf3jc72KpylW33djrNf4W6w25KqydnS3/Xs/1us0E6jppcZvY11lFVZ2hZ3Yr8W6jv8E1mD0zfZrVxpHAlb7mYl19n62bD22O+14T2GNcUyyl2/G+ke93+6uiysTQ4VtX0RaY3X1lX3+/ErplRNKOUJELgdeN8a090Bds7EG4Z6tdOOa7yME65/sdcbNL035KxH5B9Zg+XQ36/kXEGGMubvSjT1ARF4D1htjPPrFMm+jSV85xj4bXAD81xhT3hlodepqD2wAuhhjatofXVHdV2F9OjuLdUnqJKB9FbqjVDXYXToG61PTEKxPUROMMZ85Gpif0e4d5Qj7KpuTWP3Rr7lZ14tYXVjPeDrh20q/U3AUuBQYqwm/VsRidRfmYnXdPasJ3/P0TF8ppQKInukrpVQA0aSvlFIBxOtm8mvcuLFJSkqqcfnc3FyioqI8F5BS1aDtTzll/fr1x40xTSrbzuuSflJSEuvWratx+eTkZEaMGOG5gJSqBm1/yikisr8q22n3jlJKBRBN+kopFUA06SulVADRpK+UUgFEk75SSgUQTfpKKRVAvO6STaWU7ykoKmFrxikyc92bkqh7Yn2ax9bzUFSqPJr0lVLVlnO2iA37T/JDaiZr92Wy6UAWZ4tK3K43JjyEl27qxRXdzncDMuUOTfpKqUodzznLutRM1u6zEv2OQ9kUlxiCBLolxHLLoNYMbNOQxAaRSNkbQFZRfmExzyzdwd3vrufui9ryh8s7ERKsPdCepklfKfULxhjST+axdl+mdSafmsneY7kAhIcE0btlA+4d0Y4BSXH0bd2Q6HDPpZG5U4fwzJIdvLFiL5sPZPH/JvSlSUx45QVVlWnSV0oBsHz7YZZuOcQPqZkcOpUPQP2IEPonxXFjv5YMbNOQ7omxhIcE11oM4SHBPDe2B31bNeTRhVu55pWVvH5LX/onxVVeWFWJJn2lAlxxieHFz3fxxoq9NI0JZ2CbOAa2iWNAUhyd4mMICqphf40bru/Xgi7N63PP++sZP2M1j17dhTuGJSE17TtSP9Okr1QAyzlbxG8/3MSXO48wcXArnryuG6Fe0o/eNaE+H99/AQ/P3cwzS3ewPu0kf7++p0e7kwKRd7y6Sqk6l37yDDdM+56vdx3h6VHdeHZMD69J+KVi64XyxsR+/OnKzny69RBjXvuOPUdPOx2WT/OuV1gpVSfW789kzGvfkZGVx9t3DGTS0CSnQ6pQUJBwz4h2vDd5EFlnChj16ncs3XLQ6bB8liZ9pQLMgg3pTJixhujwEBbeO4wLO1Z63w2vMLRdY5Y+MJzOzWK4f/ZGnl6yncJi978bEGg06SvlBfIKinl80Tb+9slOjmbn18o+SkoMf/9sF7+bs5l+rRuy6L5htG8aXSv7qi3NYiP4cMoQ7hiWxH++S2XCjNUcqaXny1/piIhSDjt8Kp+7Zq1j28FTCPCf71O5sV8Lpl7UjpZxkR7ZR+7ZIn770Sa+2HGEXw1qxdOjvGfAtrrCQoJ48rpu9GnVkEfmb+GaV1by/yb0ZUi7Rk6H5hN881VXyk9sPpDFqFe/Ze+xHGbe1p9vHh7B9X1bMHddOiNeSuahjzbx4xH3Bi4zsvK4Yfoqvtp5hKeu68pzY7r7bMJ3NapXAovvG0ZsvVBumbma6StSMMY4HZbX0zN9pRyydMtBfj9nM01iwpn166F0blYfgL+N68FvLu3AzJV7eX9NGgs3ZnBFt3juHdGeXi0bVGsf6/ef5O5313O2sJi3bh/AiE5Na+NQHNMhPobF91/An+Zt4YVPd7F8+2Fu7N+Sq7s3JzYy1OnwvJJ423/G/v37G70xuvJVVWl/xhj++eVP/Ourn+jfuiHTb+1H4+jypxrIzC3g7e9Tefu7fWTnFzG8Q2PuHdGewW3jKv2i0qKNGfxx/haax0bw5qT+tG8aU9PD8nrGGN5fk8Z/vttHyrFcwoKDuLhzE8b2SWREp6ZEhNbet4i9hYisN8b0r2y7Kp3pi8hDwGTAAFuBO4BhwP9gdRHlALcbY/aUKZcE7AR224tWG2OmVu0QlPI/eQXFPDxvM8u2HOKGfi14bmz3805rEBcVxu8u68hdw9swe00a/165jwn/Xk3fVg247+L2XNK56TnJv6TE8PIXu3ntmxQGt41j2i39aBgVVtuH5igRYeLg1twyqBXbMrJZtCmDjzcf5PPtR4iJCOGaHs0Z3TuRQW3iHPmGsTep9ExfRBKBb4Guxpg8EZkDfAI8Cow2xuwUkXuBgcaY28uUTQKWGmO6VzUgPdNXvux87e/wqXymvLuOrRmneOTKzky5sG21pxXILyxm7vp0pienkJGVR+dmMdx3cXuu7tGc4CDhTEERD320ic+3H2HCwJY8Pao7YSG+339fE0XFJazae4KFGzP4fNthcguKSYiNYFTvRMb0Sfi5O81fePRM396unogUApHAQayz/tJnLdZeppQqx5b0LO6atY6c/CJm3Nqfy7rG16ieiNBgbh3cmvEDWrJk80FeT07hgQ828vLy3dx5QRs+XHuAXYezefzartwZ4HPVhAQHMbxDE4Z3aELemGK+2HmERRszmLlyL9NXpNC5WQxj+iQyundCQN24pdKkb4zJEJGXgDQgD1hujFkuIpOBT0QkD8gGBldQRRsR2Whv85gxZqWHYlfKJyzbcojfz91Eo6hw5t0zlC7N3T/DDA0OYlzfFozpncjyHYd57ZsUnli8nZjwEN68fQAX+9mArbvqhQUzqlcCo3olcCLnLMu2HmLhxgxe+HQXf/9sF4PaxDG2TyJX92hOTIR/DwBXpXunITAfuBnIAuYC84BxwN+NMWtE5A9AJ2PM5DJlw4FoY8wJEekHLAK6GWOyy2w3BZgCEB8f3+/DDz+s8QHl5OQQHe1bXzhR/sO1/Rlj+DilkIV7CmnfIIgH+0RQP7x2zryNMew+WUJchNA0MjC7c2riSG4Jqw8VsepgEYfPGOIihHt7h9O+ge8N/F588cVV6t6pStK/EbjSGPNr+/FtwBDgcmNMO3tZK+AzY0zXSupKBh42xlTYaa99+sqXlba//MJi/jBvC0s2H2Rcn0SeH9cjIK4g8VXGGH5IPcnv527i8Kl8Hr+2K7cObu1T3WNV7dOvyilBGjBYRCLFegYuBXYAsSLS0d7mMqyrdMoG0UREgu2/2wIdgL1VPAalfNKR7HxufmMVS7cc5E9Xdublm3ppwvdyIsLANnEsvX84wzs04YnF23noo02cKShyOjSPq0qf/hoRmQdsAIqAjcAMIB2YLyIlwEngTgARGQX0N8Y8AVwIPCMiRUAxMNUYk1krR6KUF0g9Vcwjr35Hdn4hb0zsx+V6g2+fEhsZyszb+jNtRQovL9/NjkPZTJ/Yj7ZN/KfLWL+cpZSHfLbtMA/OXk/jmAhmThpA1wT/uiQw0Hz703Ee/HAjBUUlvHRjT67s3tzpkM7Lk907SqlKZOYW8NBHm0iMCWLR/cM04fuBCzo0ZukDF9CuaTRT39vA85/spMgPpnLWpK+UB7zzfSp5hcXc1SOcpjERToejPCShQT3m3D2Y24a0ZsZ/9/KrmWs4etq3p3LWpK+Um84UFPHOqlRGdoknIVrfUv4mPCSYZ0Z3558392ZLehbXvPIta/f57tCktlCl3PTh2gNknSnknhFtnQ5F1aIxfRJZdN8wosNDmPDv1cxcudcnp3LWpK+UGwqLS3jz230MTIqjX+s4p8NRtaxzs/osvn8YI7s05dllO7lv9gZyzvrWZZ2a9JVyw5LNB8nIymOqnuUHjPoRoUyf2I9Hr+7M59uPMOrVb92+0U1d0qSvVA2VlBimr0ihU3yMznUTYESEKRe24/3Jg8jOK2L0q9+xeFOG02FViSZ9pWrom91H+fFIDlNHVH+KZOUfBrdtxLIHL6B7Yn1+8+Em/rp0B8Ul3t3Pr0lfqRqaviKFxAb1uLZngtOhKAfF149g9l2DuX1oEm9+u48HPthAfmGx02FVSJO+8gvGGB76aBNz1h2ok/2tS83kh9STTB7exi9uMq7cExocxFOjuvHYNV34ZOthJr21llN5hU6HVS5trcovrN2XycKNGTyxeBv7T+TW+v6mr0ihYWQoNw9oWev7Ur5j8vC2/Gt8bzakneSm6as4dCrP6ZDOoUlf+YVZq/ZTPyKEkKAg/rxga61eP/3jkdN8ufMok4YmERlW1ZvPqUAxuncib98xkIysPMa9/r3XXdmjSV/5vCPZ+Xy+/TA3D2jJI1d15vuUE7XazfPGir3UCw1m0pCkWtuH8m3D2jfmo7sHU1RiuGHa9171DV5N+srnzV6TRrExTBzcml8NbMXANnE8u2wnR7M9P0dKRlYeizdlcPOAljSMCvN4/cp/dEuIZcE9Q2kcE87EN9fw2bZDTocEaNJXPq6gqITZa9MY0bEJrRtFERQkvDCuB2eLSnhi8XaP7+/NlfsAmDy8jcfrVv6nZVwk86cOpVtCfe55fwOzVqU6HZImfeXbPt9+mGOnz3KbS1dL2ybR/ObSDny2/bBHz65O5hbw4Q9pjOqVQIuGkR6rV/m3hlFhzJ48mEs7N+WJxdt58bNdjs7Zo0lf+bR3V+2nVVwkF3Vs8ovlUy5sS9fm9Xl88XZOnfHMpXOzVu3nTEExd1/UziP1qcBRLyyY6RP7MWFgS15PTuHhuVsodGhufk36ymftPJTN2tRMbh3cmqCgX34jNjQ4iBdv6ElmbgHPf3LO7ZurLa+gmHdWpXJp56Z0ahbjdn0q8IQEB/H82B48NLIj8zekM/mddeQ6MFmbJn3ls2at2k94SBA39m9R7vruibFMHt6Gj9Yd4Ls9x93a15x1B8jMLWDqCD3LVzUnIvxmZAdeGNeDlT8dY8K/V3M852ydxqBJX/mkU3mFLNqYwejeCTSIrPgqmodGdiSpUSR/XrCVvIKafTW+sLiEGf/dS7/WDRmQpNMnK/eNH9iKGbf258cjp7l+2vekHq/9LxSW0qSvfNK89enkFRb/YgC3PBGhwfxtXE/SMs/wjy9212hfy7YcsqZP1r585UEju8Yz+67BZOcVcv2079mSnlUn+9Wkr3xOSYnhvdX76duqAd0TYyvdfki7RkwY2JI3v93H5gPVe2MZY02f3KFpNJd21umTlWf1bdWQefcMpV5YMONnrCZ599Fa36cmfeVzvt1znH3Hc5k0NKnKZR65qgtNYsL50/zqXTWR/OMxdh0+zd0XtTtnsFgpT2jXJJoF9wwlqVEULy3fXetTM2vSVz5n1qpUGkeHcWX3ZlUuE1svlL+O7s6uw6d5Y0VKlctNS06heWwEo3rp9Mmq9jStH8FHdw/mrUkDCK7lkwtN+sqnHMg8w1e7jjJ+QCvCQ4KrVfbybs24pkdzXvlqD3uOVj4J1oa0k6zdl8nk4W0JC9G3iqpdMRGhNK0fUev70ZasfMr7a9IIEuFXg1rVqPxTo7pRLyyYR+ZvpaSSj9HTk1OIrRfKeJ0+WfkRTfrKZ+QXFvPRD2lc1iWehAb1alRHk5hwHrumC+v2n+S9Nfsr3G7P0dMs33GESUNaExWu0ycr/6FJX/mMpVsOcfJMIbcNae1WPTf0a8HwDo35+6e7yMgq/yYXb6zYS0RoULUGi5XyBZr0lc94d1Uq7ZtGM6RdI7fqERGeH9uDEgOPLTz3hiuHTuWxaFMGN/dvSaPocLf2pZS30aSvfMKmA1lsTj/FbUNaI+L+1Q0t4yJ5+IpOfLP7GB9vPviLdW99u48SY936Til/o0lf+YRZq1KJCgtmbJ9Ej9V5+9AkerdswNNLdnDCnv/k1JlCZq9J49qezWkZp9MnK/+jSV95vRM5Z1m65RDX92tBTESox+oNDhL+fn1PTucX8szSHQC8uzqV3IJinXJB+S1N+srrfbTuAAVFJdw62L0B3PJ0ahbDvSPas3jTQT7deoj/fJfKiE5N6NK8vsf3pZQ30KSvvFpxieH91WkMbdeIDvG1M4/9vRe3o0PTaB74YCMncgv0LF/5NU36yqt9vesoGVl5bl+meT7hIcG8cH1Pio2hT6sGDGqj0ycr/6XfOlFebdaqVJrHRjCyS3yt7qdf64a8c8dA2jSO8sjVQUp5K036ymvtPZbDyp+O8/DlHQkJrv0PpReWuc+uUv5Iu3eU13p39X5Cg4WbB9Rsnh2l1Lk06SuvlHu2iHnr0rm6R3OaxOi3YpXylColfRF5SES2i8g2EflARCJE5FIR2SAim0TkWxFpX0HZP4vIHhHZLSJXeDZ85a8Wbcrg9NmiSm+HqJSqnkqTvogkAg8C/Y0x3YFgYDwwDbjFGNMbmA08Vk7Zrva23YArgddFpHqToKuAY4zh3VX76ZZQn76tGjgdjlJ+pardOyFAPREJASKBg4ABSr/BEmsvK2s08KEx5qwxZh+wBxjoXsjK363dl8muw6eZNCRJr6RRysMqvXrHGJMhIi8BaUAesNwYs1xEJgOfiEgekA0MLqd4IrDa5XG6vewXRGQKMAUgPj6e5OTk6h7Hz3Jyctwqr5z3+qZ8okIhNnsPyclVv7WhN9D2p7xdpUlfRBpinbG3AbKAuSIyERgHXG2MWSMifwD+AUwuW7ycKs+5XZExZgYwA6B///5mxIgR1TmGX0hOTsad8spZR7Lz2bD8a+4Y1oYrLu3qdDjVpu1PebuqdO+MBPYZY44ZYwqBBcAwoJcxZo29zUfA0HLKpgOu95prQfndQEoBMHtNGsXGMLEW5tlRSlUt6acBg0UkUqwO1kuBHUCsiHS0t7kM2FlO2Y+B8SISLiJtgA7AWg/ErfxQQVEJs9emMaJjE1o3inI6HKX8UlX69NeIyDxgA1AEbMTqikkH5otICXASuBNAREZhXenzhDFmu4jMwfonUQTcZ4wprp1DUb7u8+2HOXb6rF6mqVQtqtI0DMaYJ4EnyyxeaP+U3fZjrDP80sfPAc+5EaMKAMYYZq1KpVVcJBfpdAhK1Rr9Rq7yCl/sOMIPqSe5c1gSQUF6maZStUWTvnJcXkExTy/ZQcf4aG7RAVylapXOsqkcNy15DxlZeXw4ZTChdTCbplKBTN9hylH7T+Qy/b97Gd07gcFtGzkdjlJ+T5O+cowxhqc+3k5okPDo1V2cDkepgKBJXznmy51H+Wb3MR66rCPx9SOcDkepgKBJXzkiv7CYp5dsp2N8NJOGJjkdjlIBQwdylSNeT04h/WQeH9ylg7dK1SV9t6k6t/9ELtNXpDCqVwJD2ungrVJ1SZO+qnPPLNlBaJDwl2t08FapuqZJX9WpL3cc4atdR/ntSB28VcoJmvRVnckvLObppdvp0DSa24clOR2OUgFJB3JVnZmWnMKBzDxm3zVIB2+Vcoi+81SdSDtxhmkrUriuVwJD2zV2OhylApYmfVUnnllqffP2L/rNW6UcpUlf1bqvdh7hy51HefDSDjSL1cFbpZykSV/VqvzCYp5asp32TaO5Y1gbp8NRKuDpQK6qVdNX2IO3kwcRFqLnGEo5Td+FqtYcyDzDtOQUru3ZnKHtdfBWKW+gSV/VmqeX7CBYv3mrlFfRpK9qxde7jvDlziM8eGkHmsfWczocpZRNk77yuPzCYp76eAftmkRxpw7eKuVVdCBXedwbK/aSlnmG93XwVimvo+9I5VEHMs/wevIerunRnGE6eKuU19GkrzzqmaXW4O1j1+rgrVLeSJO+8phvdh3lix1HeOASHbxVyltp0lcecTq/kCc/3k7bJlH8+gIdvFXKW+lArnKbMYZH5m8lI8u6560O3irlvfTdqdz2zvepLNt6iIcv78TANnFOh6OUOg9N+sotG9NO8twnO7m0c1PuvrCt0+EopSpXNaa2AAAP8ElEQVShSV/V2MncAu6fvZH4+hG8fFMvgoLE6ZCUUpXQPn1VIyUlht/N2cSx02eZd88QGkSGOR2SUqoK9Exf1ci0FSl8s/sYj1/bhZ4tGjgdjlKqijTpq2r7PuU4Ly/fzXW9Epg4uLXT4SilqkGTvqqWo9n5PPjBJpIaR/G3cT0Q0X58pXyJ9umrKisqLuGBDzaSe7aI2XcNIjpcm49SvkbftarK/vHFj6zZl8k/bupFx/gYp8NRStWAdu+oKvl61xFeT05hwsCWjOvbwulwlFI1pElfVSr95Bke+mgzXZvX58nrujkdjlLKDVXq3hGRh4DJgAG2AncAXwCln/GbAmuNMWPKKVtslwFIM8aMcjdoVXfOFhVz3/sbKCkxTJvYl4jQYKdDUkq5odKkLyKJwINAV2NMnojMAcYbY4a7bDMfWFxBFXnGmN4eiVbVueeX7WRz+immT+xL60ZRToejlHJTVbt3QoB6IhICRAIHS1eISAxwCbDI8+EpJy3ZfJB3Vu1n8gVtuLJ7c6fDUUp5QKVn+saYDBF5CUgD8oDlxpjlLpuMBb4yxmRXUEWEiKwDioAXjDHn/HMQkSnAFID4+HiSk5OrdxQucnJy3CqvLIdySnh6VR7tGwQxOPIIyclHnQ7JJ2j7U96uKt07DYHRQBsgC5grIhONMe/Zm0wAZp6nilbGmIMi0hb4WkS2GmNSXDcwxswAZgD079/fjBgxovpHYktOTsad8gryCooZ89p3REaE8e49F+hdsKpB25/ydlXp3hkJ7DPGHDPGFAILgKEAItIIGAgsq6iwMeag/XsvkAz0cTNmVYuMMTy2aBs/Hj3NP2/urQlfKT9TlaSfBgwWkUixvnN/KbDTXncjsNQYk19eQRFpKCLh9t+NgWHADvfDVrVlzroDzN+QzgOXdODCjk2cDkcp5WGVJn1jzBpgHrAB69LLIOyuGGA88IHr9iLSX0RKu3u6AOtEZDPwDVafviZ9L7XjYDZPLN7OBe0b85tLOzgdjlKqFlTpOn1jzJPAk+UsH1HOsnVY1/RjjPke6OFeiKouFJcY7p+9gQaRofxzfG+C9YYoSvkl/UauAuC7PcfZezyXx67pSuPocKfDUUrVEk36CoCFGzOoHxHCZV3jnQ5FKVWLNOkrcs8W8dm2w1zTM0GnWVDKz2nSV3y27TB5hcVc3zfR6VCUUrVMk75i4cYMWsVF0q91Q6dDUUrVMk36Ae7QqTy+SznOmD6JeutDpQKAJv0At3jTQYyBcX20a0epQKBJP4AZY1iwIZ2+rRqQ1FinTVYqEGjSD2DbD2bz45EcxurtD5UKGJr0A9jCjRmEBgvX9dS58pUKFJr0A1RRcQmLNx3kks5NaRAZ5nQ4Sqk6okk/QK3cc5zjOWcZ20e7dpQKJJr0A9TCDRk0iAzl4s46fbJSgUSTfgA6nV/I8h2HubZnc8JDdNoFpQKJJv0A9Om2w+QXlmjXjlIBSJN+AFq4IYOkRpH0bdXA6VCUUnVMk36AycjKY/W+E4zt00KnXVAqAGnSDzCLNmZgDIzVaReUCkia9AOIMYaFGzMYkNSQVo0inQ5HKeUATfoBZFtGNnuO5ugArlIBTJN+AJm/IZ2wkCCu6aHTLigVqDTpB4jC4hKWbD7IyC5NiY0MdTocpZRDNOkHiJU/HeNEboF27SgV4DTpB4j5GzKIiwrjoo467YJSgUyTfgA4lVfIFzuOcF3P5oSF6EuuVCDTDBAAPt16iIKiEr1ZilJKk34gWLAxg7ZNoujVItbpUJRSDtOk7+cOZJ5h7b5MxvVJ1GkXlFKa9P3doo0ZAIzurdMuKKU06fu10mkXBrWJo2WcTruglNKk79c2p59i7/FcxvXVs3yllEWTvh9bsCGd8JAgrtJpF5RSNk36fqqgyJp24bKu8dSP0GkXlFIWTfp+asWPxzh5plC7dpRSv6BJ308t2JBO4+gwhnfQaReUUv9Hk74fOnWmkK92HuW6XgmEButLrJT6P5oR/NCyrYcoKC5hnM6oqZQqQ5O+H1qwIZ0OTaPpnljf6VCUUl5Gk76fSTtxhnX7TzK2r067oJQ6V5WSvog8JCLbRWSbiHwgIhEislJENtk/B0VkUQVlJ4nIT/bPJM+Gr8pauDEDERij0y4opcoRUtkGIpIIPAh0NcbkicgcYLwxZrjLNvOBxeWUjQOeBPoDBlgvIh8bY0566gDU/zHGsGBjOkPaNiKhQT2nw1FKeaGqdu+EAPVEJASIBA6WrhCRGOASoLwz/SuAL4wxmXai/wK40r2QVUU2pGWx/8QZxvbRs3ylVPkqPdM3xmSIyEtAGpAHLDfGLHfZZCzwlTEmu5ziicABl8fp9rJfEJEpwBSA+Ph4kpOTq3wAZeXk5LhV3pfN2n6WsCCIztpDcnKK0+EEpEBuf8o3VKV7pyEwGmgDZAFzRWSiMeY9e5MJwMyKipezzJyzwJgZwAyA/v37mxEjRlQeeQWSk5Nxp7yv2nM0hx+Sv+OqnglcNbKP0+EErEBtf8p3VKV7ZySwzxhzzBhTCCwAhgKISCNgILCsgrLpQEuXxy1w6RpSnrHjYDY3v7GK8JBgHrikg9PhKKW8WFWSfhowWEQixboG8FJgp73uRmCpMSa/grKfA5eLSEP7E8Pl9jLlIRvTTjJ+xirCQoKYc/dg2jeNdjokpZQXqzTpG2PWAPOADcBWu8wMe/V44APX7UWkv4jMtMtmAn8FfrB/nrGXKQ9YvfcEE2euoWFUGHPuHkLbJprwlVLnV2mfPoAx5kmsSy/LLh9RzrJ1wGSXx28Bb9U8RFWe5N1Hufvd9bSMi+T9yYOIrx/hdEhKKR9QpaSvvMtn2w7zwAcb6NA0hnd/PZBG0eFOh6SU8hGa9H3Moo0Z/H7uZnq2iOXtOwYSW09vkKKUqjpN+j5k9po0/rJoK4PbNGLmpP5EhevLp5SqHs0aPmLmyr08u2wnF3dqwrSJ/YgIDXY6JKWUD9Kk7+WMMbz69R5e/uJHrurejH+N70NYiE6OqpSqGU36XswYw98/2830FSmM65PIizf0JETvhKWUcoPfJP38wmL++eVPtDMlTofiESUlhqeWbGfWqv3cMqgVfx3dnaAgnR9fKeUevzltPJFbwHur9/PWtrOUlJwzvY9PKS4x/HH+Fmat2s9dw9vw7BhN+Eopz/CbpJ/YoB5/uaYLOzNLeH/NfqfDqbGCohIe/HAj89an89uRHXj06i56ByyllMf4TdIHGD+gJd0bBfP8J7tIO3HG6XCqLb+wmHveW8+yLYd49OrO/HZkR034SimP8qukLyLc0T2MkCDh4Xmbfaqbp6CohLtmreOrXUf565juTLmwndMhKaX8kF8lfYBG9YJ4/LqurN2XyaxVqU6HU2XPLN3Oyp+O8+L1Pbl1cGunw1FK+Sm/S/oAN/ZrwcWdmvDCZ7tIPZ7rdDiV+uiHNN5bncbdF7blpgEtKy+glFI15JdJX0T427iehAYH8fDczRR7cTfPxrSTPL5oO8M7NOaPV3Z2OhyllJ/zy6QP0Cw2gqeu68a6/Sf5z3f7nA6nXEdP5zP1vfXEx4bzyvg+BOtlmUqpWua3SR9gXN9ERnZpyv98vpu9x3KcDucXCopKuO/9DZzKK+SNif1pGBXmdEhKqQDg10lfRHh+bA8iQoO9rpvnr0t38EPqSV68oRddE+o7HY5SKkD4ddIHaFo/gqdHdWNDWhZvfrvX6XAAmPPDAd5dvZ8pF7ZlVK8Ep8NRSgUQv0/6AKN7J3B513heWv4je46edjSWTQeyeGzRNoa1b8Qfr+jkaCxKqcATEElfRHhubA+iwoL5/dwtFBU7MynbsdNnmfrueprWD+fVCX11xkylVJ0LmKzTJCacZ0Z3Z/OBLP69su6v5ikduM3KK+CNW/vpwK1SyhEBk/QBru3ZnKt7NON/v/iRH4/UbTfPc8t2sDY1k79f35NuCbF1um+llCoVUElfRHhmdHeiI0J4eO7mOuvmmbvuAO+s2s/kC9owundinexTKaXKE1BJH6BxdDh/Hd2dLemneOO/tX81z+YDWfxl0TaGtmvEI1fpN26VUs4KuKQPcE3P5lzbszn//PJHdh3OrrX9HDt9lqnvradJdDiv/koHbpVSzgvYLPTM6O7E1gvl93M2U1gL3TyFxSXcN3sDmbnWwG2cDtwqpbxAwCb9uKgwnh3Tg+0Hs5mWnOLx+p9btpO1+6yB2+6JOnCrlPIOAZv0Aa7s3ozRvRN45auf2H7wlMfqnbc+nbe/T+XXF7RhTB8duFVKeY+ATvoAT13XjYZRYTw8dwsFRe5382xJz+LRhVsZ0rYRf9aBW6WUlwn4pN8wKoznx/Zg56FsXv1mj1t1Hc+xvnFrDdz20YFbpZTXCXE6AG9wWdd4xvVN5PVv9pB2IpegGt6MfMehbE7kFjD/nqE0ig73cJRKKeU+Tfq2J6/txqGsfNannaxxHaFBQfzjpt46cKuU8lqa9G2xkaF8MGWw02EopVSt0k5npZQKIJr0lVIqgGjSV0qpAKJJXymlAogmfaWUCiCa9JVSKoBo0ldKqQCiSV8ppQKIGGOcjuEXROQU8NN5NokFzjclZmPguEeDqluVHZ+378/d+qpbvjrbV2Vbd7fR9ufs/uq6/VWnjKe2q2h9a2NMk0prN8Z41Q8ww83165w+hto8fm/fn7v1Vbd8dbavyrbubqPtz9n91XX7q04ZT23n7jF6Y/fOEjfX+7q6Pj5P78/d+qpbvjrbV2VbT23jq7T91V4ZT23n1jF6XfeOu0RknTGmv9NxqMCk7U95O28803fXDKcDUAFN25/yan53pq+UUqpi/nimr5RSqgKa9JVSKoBo0ldKqQASUElfRKJEZL2IXOt0LCrwiEgXEZkuIvNE5B6n41GBySeSvoi8JSJHRWRbmeVXishuEdkjIo9Uoao/AXNqJ0rlzzzRBo0xO40xU4GbAL2sUznCJ67eEZELgRxgljGmu70sGPgRuAxIB34AJgDBwN/KVHEn0BPrK/IRwHFjzNK6iV75A0+0QWPMUREZBTwCvGqMmV1X8StVyidujG6M+a+IJJVZPBDYY4zZCyAiHwKjjTF/A87pvhGRi4EooCuQJyKfGGNKajVw5Tc80Qbtej4GPhaRZYAmfVXnfCLpVyAROODyOB0YVNHGxpi/AIjI7Vhn+prwlbuq1QZFZAQwDggHPqnVyJSqgC8nfSlnWaV9VcaYtz0figpQ1WqDxphkILm2glGqKnxiILcC6UBLl8ctgIMOxaICk7ZB5XN8Oen/AHQQkTYiEgaMBz52OCYVWLQNKp/jE0lfRD4AVgGdRCRdRH5tjCkC7gc+B3YCc4wx252MU/kvbYPKX/jEJZtKKaU8wyfO9JVSSnmGJn2llAogmvSVUiqAaNJXSqkAoklfKaUCiCZ9pZQKIJr0lVIqgGjSV0qpAKJJXymlAsj/B09sJ7R5PULfAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.semilogx(regul_val, accuracy_val)\n",
    "plt.grid(True)\n",
    "plt.title(\"Test accuracy by regularization (logistic)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if the same technique will improve the prediction of the 1-layer neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected character after line continuation character (<ipython-input-9-49bbbdf8fb7a>, line 28)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-49bbbdf8fb7a>\"\u001b[0;36m, line \u001b[0;32m28\u001b[0m\n\u001b[0;31m    beta_regul * (tf.nn.l2_loss(weights_l1) + \\\u001b[0m\n\u001b[0m                                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "H = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Input\n",
    "    tf_train_dataset = tf.placeholder(\n",
    "        tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta_regul = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # Variables\n",
    "    weights_l1 = tf.Variable(tf.truncated_normal([image_size * image_size, H]))\n",
    "    biases_l1 = tf.Variable(tf.zeros([H]))\n",
    "    weights_l2 = tf.Variable(tf.truncated_normal([H, num_labels]))\n",
    "    biases_l2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    # Training computation\n",
    "    train_l1 = tf.nn.relu(tf.matmul(tf_train_dataset, weights_l1) + biases_l1)\n",
    "    logits = tf.matmul(train_l1, weights_l2) + biases_l2\n",
    "    loss = tf.reduce_mean( \\\n",
    "        tf.nn.softmax_cross_entropy_with_logits( \\\n",
    "            logits=logits, labels=tf_train_labels)) + \\\n",
    "                beta_regul * (tf.nn.l2_loss(weights_l1) + \\ \n",
    "                                  tf.nn.l2_loss(weights_l2))\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    # Prediction for the training, validation and test data\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_l1 = tf.nn.relu(tf.matmul(tf_valid_dataset, weights_l1) + biases_l1)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(valid_l1, weights_l2) + \\\n",
    "                                     biases_l2)\n",
    "    test_l1 = tf.nn.relu(tf.matmul(tf_test_dataset, weights_l1) + biases_l1)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(test_l1, weights_l2) + biases_l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels,\\ \n",
    "                 beta_regul : 1e-3}\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction], \\\n",
    "                                    feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), \\\n",
    "                                                     valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 3001\n",
    "regul_val = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "accuracy_val = []\n",
    "\n",
    "for regul in regul_val:\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.initialize_all_variables().run()\n",
    "        for step in range(num_steps):\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            \n",
    "            feed_dict = {tf_train_dataset: batch_data, \n",
    "                         tf_train_labels: batch_labels, beta_regul: regul}\n",
    "            _, l, predictions = session.run([optimizer, loss, \\\n",
    "                                             train_prediction], \\ \n",
    "                                                feed_dict=feed_dict)\n",
    "        accuracy_val.append(accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogx(regul_val, accuracy_val)\n",
    "plt.grid(True)\n",
    "plt.title(\"Test accuracy by regularization (logistic)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "H = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Input\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, \\\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta_regul = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # Variables\n",
    "    weights_l1 = tf.Variable(tf.truncated_normal([image_size * image_size, H]))\n",
    "    biases_l1 = tf.Variable(tf.zeros([H]))\n",
    "    weights_l2 = tf.Variable(tf.truncated_normal([H, num_labels]))\n",
    "    biases_l2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    # Training computation\n",
    "    train_l1 = tf.nn.relu(tf.matmul(tf_train_dataset, weights_l1) + biases_l1)\n",
    "    logits = tf.matmul(train_l1, weights_l2) + biases_l2\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits( \\\n",
    "            logits=logits, labels=tf_train_labels)) + \\\n",
    "                beta_regul * (tf.nn.l2_loss(weights_l1) + \\\n",
    "                                  tf.nn.l2_loss(weights_l2))\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    # Prediction for the training, validation and test data\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_l1 = tf.nn.relu(tf.matmul(tf_valid_dataset, weights_l1) + biases_l1)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(valid_l1, weights_l2) + \n",
    "                                     biases_l2)\n",
    "    test_l1 = tf.nn.relu(tf.matmul(tf_test_dataset, weights_l1) + biases_l1)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(test_l1, weights_l2) + biases_l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 101\n",
    "num_batches = 3\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    #offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    offset = ((step % num_batches) * batch_size) % \\\n",
    "                (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, \\\n",
    "                 tf_train_labels : batch_labels, beta_regul : 0.0}\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction], \\\n",
    "                                    feed_dict=feed_dict)\n",
    "    if (step % 2 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), \\\n",
    "                                                     valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are far too much parameters and no regularization, the accuracy of the batches is 100%. The generalization capability is poor, as shown in the validation and test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "H = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Input\n",
    "    tf_train_dataset = tf.placeholder(\n",
    "        tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta_regul = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # Variables\n",
    "    weights_l1 = tf.Variable(tf.truncated_normal([image_size * image_size, H]))\n",
    "    biases_l1 = tf.Variable(tf.zeros([H]))\n",
    "    weights_l2 = tf.Variable(tf.truncated_normal([H, num_labels]))\n",
    "    biases_l2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    # Training computation\n",
    "    train_l1 = tf.nn.relu(tf.matmul(tf_train_dataset, weights_l1) + biases_l1)\n",
    "    drop1 = tf.nn.dropout(train_l1, 0.5)\n",
    "    logits = tf.matmul(train_l1, weights_l2) + biases_l2\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits( \\\n",
    "            logits=logits, labels=tf_train_labels)) + \\\n",
    "                beta_regul * (tf.nn.l2_loss(weights_l1) + \\\n",
    "                                  tf.nn.l2_loss(weights_l2))\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    # Prediction for the training, validation and test data\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_l1 = tf.nn.relu(tf.matmul(tf_valid_dataset, weights_l1) + biases_l1)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(valid_l1, weights_l2) + \n",
    "                                     biases_l2)\n",
    "    test_l1 = tf.nn.relu(tf.matmul(tf_test_dataset, weights_l1) + biases_l1)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(test_l1, weights_l2) + biases_l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 101\n",
    "num_batches = 3\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = ((step % num_batches) * batch_size) % \\\n",
    "                (train_labels.shape[0] - batch_size)\n",
    "    #offset = step % num_batches\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, \\\n",
    "                 tf_train_labels : batch_labels, beta_regul : 0.0}\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction], \\\n",
    "                                    feed_dict=feed_dict)\n",
    "    if (step % 2 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), \\\n",
    "                                                     valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no improvement, something is fishy..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use 2 layers, with a slightly different parameters inizialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "H1 = 1024\n",
    "H2 = 512\n",
    "beta_regul = 1e-3\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Input\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, \\\n",
    "                                     shape=(batch_size, \\\n",
    "                                            image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    global_step = tf.Variable(0)\n",
    "    \n",
    "    # Variable\n",
    "    weights_l1 = tf.Variable(tf.truncated_normal([image_size * image_size, H1],\\\n",
    "            stddev=np.sqrt(2.0 / (image_size * image_size))))\n",
    "    biases_l1 = tf.Variable(tf.zeros([H1]))\n",
    "    weights_l2 = tf.Variable(tf.truncated_normal([H1, H2], \\\n",
    "            stddev=np.sqrt(2.0 / H1)))\n",
    "    biases_l2 = tf.Variable(tf.zeros([H2]))\n",
    "    weights_l3 = tf.Variable(tf.truncated_normal([H2, num_labels], \\\n",
    "            stddev=np.sqrt(2.0 / H2)))\n",
    "    biases_l3 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    # Training\n",
    "    train_l1 = tf.nn.relu(tf.matmul(tf_train_dataset, weights_l1) + biases_l1)\n",
    "    train_l2 = tf.nn.relu(tf.matmul(train_l1, weights_l2) + biases_l2)\n",
    "    logits = tf.matmul(train_l2, weights_l3) + biases_l3\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits=logits, \\\n",
    "                                                labels=tf_train_labels) + \\\n",
    "        beta_regul * (tf.nn.l2_loss(weights_l1) + \\\n",
    "                      tf.nn.l2_loss(weights_l2) + \\\n",
    "                      tf.nn.l2_loss(weights_l3)))\n",
    "    \n",
    "    # Optimizer\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, 1000, 0.65, \\\n",
    "                                               staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer( \\\n",
    "        learning_rate).minimize(loss,global_step=global_step)\n",
    "    \n",
    "    # Predictions\n",
    "    \n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_l1 = tf.nn.relu(tf.matmul(tf_valid_dataset, weights_l1) + biases_l1)\n",
    "    valid_l2 = tf.nn.relu(tf.matmul(valid_l1, weights_l2) + biases_l2)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(valid_l2, weights_l3) + \\\n",
    "                                     biases_l3)\n",
    "    test_l1 = tf.nn.relu(tf.matmul(tf_test_dataset, weights_l1) + biases_l1)\n",
    "    test_l2 = tf.nn.relu(tf.matmul(test_l1, weights_l2) + biases_l2)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(test_l2, weights_l3) + biases_l3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3.256059\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 40.0%\n",
      "Minibatch loss at step 500: 1.162595\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 85.8%\n",
      "Minibatch loss at step 1000: 0.900569\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 1500: 0.670577\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.1%\n",
      "Minibatch loss at step 2000: 0.786530\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 2500: 0.530741\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 3000: 0.456223\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 3500: 0.462923\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 4000: 0.616204\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 4500: 0.403161\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 5000: 0.505463\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 5500: 0.395441\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 6000: 0.521252\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 6500: 0.394354\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 7000: 0.436181\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 7500: 0.387797\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 8000: 0.418575\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 8500: 0.393681\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 9000: 0.331544\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 90.3%\n",
      "Test accuracy: 95.7%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 9001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate minibatch\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        \n",
    "        feed_dict = {tf_train_dataset: batch_data, \\\n",
    "                     tf_train_labels: batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], \\\n",
    "                                       feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "          print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "          print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, \\\n",
    "                                                        batch_labels))\n",
    "          print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "              valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), \\\n",
    "                                             test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we improve? We should try dropout."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
